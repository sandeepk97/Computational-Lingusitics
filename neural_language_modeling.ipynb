{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Going beyond n-gram language models with smoothing and neural networks\n",
    "\n",
    "* The sparsity problem and Zipf's for n-gram language models means bigger and bigger sequences are each less and less likely\n",
    "* Many linguistic events are unseen in test corpora (say, if we switch genres)\n",
    "* Do not want to assign 0% probability to unseen events that actually really occur\n",
    "\n",
    "## Out-of-vocabulary symbols\n",
    "\n",
    "* We could learn a special \"OOV\" (out-of-vocabulary) word that we apply to all words we see less than $n$ times\n",
    "* Replace all rare words (defined by that threshold $n$) with \"OOV\"\n",
    "* Re-compute probabilities and assign $p(w = OOV)$ or $p(w_{i-1} OOV)$ etc. to non-indexed words\n",
    "\n",
    "## Smoothing | Discounting | Backoff\n",
    "\n",
    "Assuming we have a total vocabulary of size $|V|$. Following set notation, $|V|$ represents the size (cardinality) of the set of all the words we have seen, we can change the probability of any event to be more or less depending.\n",
    "\n",
    "* Laplace/Add-One smoothing\n",
    "  * We assume we have seen everything one more time than we have actually seen it\n",
    "  * Assigns $\\frac{1}{|V|}$ probability to unseen events\n",
    "  * Affects low-probability events much more than high-probability events\n",
    "* Add-k smoothing\n",
    "  * Use a smaller value $k$ that is less than 1 to allocate slightly less probability mass to unseen events\n",
    "  * $\\large\\frac{P(w_{i-1}w_i + k)}{P(w_{i-1}) + k|V|}$\n",
    "  * Choose $k$ by looking at held-out corpora\n",
    "  * Make sure to divide the denominator (e.g., your total count) by the original frequency $F$ and add $k|V|$)\n",
    "  * Generally doesn't allocate discounts well\n",
    "* Katz backoff\n",
    "  * We learn a set of coefficients $\\lambda$ that tell us what proportion to consider ngrams of smaller sizes -- such as how much to \"count\" what we have seen from bigrams and unigrams before\n",
    "  * $\\hat{P}(w_n|w_{nâˆ’2}w_{nâˆ’1}) = \\lambda_{1}P(w_n) + \\lambda_{2}P(w_n |w_{nâˆ’1}) + \\lambda_{3}P(w_n |w_{nâˆ’2} w_{nâˆ’1})$\n",
    "* Kneser-Ney smoothing\n",
    "  * Looks at the _histories_ of the words we are considering backing off too\n",
    "  * Deprioritizes guesses of frequent words that occur in very few contexts (e.g., the _Kong_ in Hong Kong)\n",
    "  * If those words occur in a variety of contexts, it form a more likely novel bigram\n",
    "  * Advanced technique\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a neural language model?\n",
    "\n",
    "* Any model that uses neural network methods to estimate the probabilities of a text\n",
    "* May use a variety of architectures\n",
    "  * word2vec\n",
    "  * recurrent neural networks (RNNs)\n",
    "  * Transformers (e.g., BERT)\n",
    "  * Convolutional neural networks (CNNs)\n",
    "  * Generative adversarial networks (GANs)\n",
    "\n",
    "## How do NLMs learn?\n",
    "\n",
    "* We tokenize language into units that we ask the model to predict\n",
    "* Tokens can vary in their size from single characters (CharRNNs) to words\n",
    "* We _optimize_ models to predict language from the context\n",
    "  * Next word prediction (e.g., GPT-2, RNNs)\n",
    "  * Masked word prediction (masked language modeling, e.g., BERT)\n",
    "* Models are penalized for making mistakes (loss)\n",
    "* Typically try to minimize the _cross-entropy_ (which depends on the _probabilities_ models assign to outcomes)\n",
    "\n",
    "## What do NLMs learn?\n",
    "* Long-distance co-occurrence statistics\n",
    "  * Critical for idiomatic expressions\n",
    "* Similarity in meaning between related words\n",
    "  * Variants of the same word: is/was/were/been or go/went/gone\n",
    "  * Words that appear in similar contexts: dog, cat, etc.\n",
    "* Latent categories of words (nouns, verbs, adverbs, etc.)\n",
    "\n",
    "## What does their ability to learn tell us about language?\n",
    "* Language processing involves combining lots of _constraints_\n",
    "* Predicting language is harder when we throw away statistical cues\n",
    "* N-gram language models are not sufficient to characterize linguistic statistics\n",
    "* (Probably neural language models also are not)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "Using a more complex neural structure that holds onto prior states in memory, we can learn contextual word representations.\n",
    "* The model uses _recurrence_ (storing the previous state) and the current input to predict the next event -- e.g., a word. \n",
    "* The hidden state keeps track of everything up to the current word\n",
    "* Very sophisticated n-gram language model\n",
    "* State-of-the-art framework under the `seq2seq` banner until about 2015\n",
    "\n",
    "<center><img src=\"images/elman_rnn.png\" width=400></center>\n",
    "\n",
    "* The hidden state and the current input are combined like so:\n",
    "  * Hidden state: $1 \\times n\\_dim$\n",
    "  * Recurrent units: $1 \\times n\\_dim$\n",
    "  * Weight matrix: $2 * n\\_dim \\times |V|$\n",
    "  * Predictions are a $1 \\times |V|$ vector where each dimension is the relative strength of that word\n",
    "  * ðŸš¨ MUST pass predictions through a `Softmax` function to obtain probabilities ðŸš¨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked language modeling objective in Transformers\n",
    "\n",
    "* Generalizes the language modeling task using a much more complex neural network architecture.\n",
    "  \n",
    "* Prediction task similar to the RNN but everything is computed in parallel, rather than serially\n",
    "* The model's goal is to predict a hidden word (or some proportion of hidden words) given the context that surrounds those words. The model is trained by _backpropagation_, so, the model gets better by making mistakes about prediction, but as the model learns, it will make fewer and fewer mistakes.\n",
    "  * **Prediction task**: The model's goal is to predict a hidden word (or some proportion of hidden words) given the context that surrounds those words.\n",
    "  * The sentences it will get kind of look like this:\n",
    "    * Masked sentence: Dr. Jacob's [MASK] Radish is very sweet and purrs very loudly.\n",
    "    * Correct sentence: [MASK] corresponds to the word \"cat.\"\n",
    "  * **Error-driven**: The model is trained by _backpropagation_, so, the model gets better by making mistakes about prediction, but as the model learns, it will make fewer and fewer mistakes. \n",
    "  \n",
    "* _Attention_ mechanism allows every token to influence every other token\n",
    "\n",
    "* This added complexity makes the model much better at guessing a word given the context, because it can incorporate _syntactic_, _semantic_, and _lexical_ knowledge, in addition to some propositional knowledge. \n",
    "\n",
    "### DistilBERT\n",
    "\n",
    "* A tiny **Transformer** whose goal was to compress a much bigger Transformer -- it is faster than the bigger model it was based off of (BERT) and is 40% of the original size.\n",
    "* \"Models\" a neural language model\n",
    "\n",
    "### The core components of Transformer models like BERT, GPT-2, XLM, etc.\n",
    "\n",
    "* Word embeddings (hidden states within the models)\n",
    "* Predicted probabilities in the outputs for each word in our vocabulary\n",
    "* \"Attention\" values that represent the strength of a link between every word to every other word in the input\n",
    "* Each of these is produced at every single **layer** in the network\n",
    "* All of this is implemented as matrices -- which we will cover in more depth later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
